import os
import torch
import cv2
import numpy as np
import segmentation_models_pytorch as smp
import albumentations as albu
from torch.utils.data import DataLoader
from torch.utils.data import Dataset as BaseDataset

# ================= 1. å…¨å±€é…ç½® =================
DATA_DIR = './dataset/camvid'  # åªè¯»å– CamVid
ENCODER = 'mobilenet_v2'
ENCODER_WEIGHTS = 'imagenet'
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

BATCH_SIZE = 4  # æ˜¾å­˜å¤Ÿå¤§å¯ä»¥æ”¹ 8
LR = 0.0001
EPOCHS = 100  # å»ºè®®è·‘ 100 è½®
INPUT_HEIGHT = 384
INPUT_WIDTH = 480

# === ä¿®æ”¹ç‚¹ 0: ç»™æ–°æ¨¡å‹èµ·ä¸ªæ–°åå­—ï¼Œé¿å¼€æ—§æ–‡ä»¶ ===
MODEL_SAVE_PATH = './best_model_camvid_5classes.pth'

# ================= 2. å…³é”®ï¼šä¿®æ­£åçš„ç±»åˆ«æ˜ å°„ =================
# ç›®æ ‡: 0=èƒŒæ™¯, 1=è·¯, 2=äºº, 3=è½¦, 4=éª‘è¡Œè€…(æ–°å¢)
CAMVID_MAPPING = {
    'road': 1, 'lane_marking_driving': 1,
    'pedestrian': 2, 'child': 2,

    # === ä¿®æ”¹ç‚¹ 1: æŠŠéª‘è½¦çš„äººå•ç‹¬åˆ†å‡ºæ¥ (ID=4) ===
    'bicyclist': 4,
    # 'motorcyclist': 4, # å¦‚æœæœ‰æ‘©æ‰˜è½¦ä¹ŸåŠ è¿™é‡Œ

    'car': 3, 'truck': 3, 'bus': 3, 'train': 3, 'heavy_vehicle': 3, 'pickup_truck': 3, 'van': 3
}

CAMVID_CLASSES = ['sky', 'building', 'pole', 'road', 'pavement',
                  'tree', 'signsymbol', 'fence', 'car', 'pedestrian', 'bicyclist', 'unlabelled']


# ================= 3. æ•°æ®é›†ç±» (å«å†…å­˜åŠ é€Ÿ) =================
class CamVidDataset(BaseDataset):
    def __init__(self, images_dir, masks_dir, augmentation=None, preprocessing=None):
        self.ids = os.listdir(images_dir)
        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]
        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]
        self.augmentation = augmentation
        self.preprocessing = preprocessing

        # å†…å­˜ç¼“å­˜
        self.images_cache = [None] * len(self.ids)
        self.masks_cache = [None] * len(self.ids)

    def __getitem__(self, i):
        # 1. æŸ¥ç¼“å­˜
        if self.images_cache[i] is not None:
            image = self.images_cache[i]
            mask = self.masks_cache[i]
        else:
            image = cv2.imread(self.images_fps[i])
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            mask = cv2.imread(self.masks_fps[i], 0)

            image = cv2.resize(image, (INPUT_WIDTH, INPUT_HEIGHT), interpolation=cv2.INTER_LINEAR)
            mask = cv2.resize(mask, (INPUT_WIDTH, INPUT_HEIGHT), interpolation=cv2.INTER_NEAREST)

            self.images_cache[i] = image
            self.masks_cache[i] = mask

        # 2. æ˜ å°„æ ‡ç­¾
        target_mask = np.zeros((INPUT_HEIGHT, INPUT_WIDTH), dtype=np.longlong)
        for class_name, target_id in CAMVID_MAPPING.items():
            if class_name in CAMVID_CLASSES:
                src_id = CAMVID_CLASSES.index(class_name)
                target_mask[mask == src_id] = target_id

        # 3. å¢å¼º
        if self.augmentation:
            sample = self.augmentation(image=image, mask=target_mask)
            image, target_mask = sample['image'], sample['mask']

        # 4. é¢„å¤„ç†
        if self.preprocessing:
            sample = self.preprocessing(image=image, mask=target_mask)
            image, target_mask = sample['image'], sample['mask']

        return image, target_mask

    def __len__(self):
        return len(self.ids)


def get_training_augmentation():
    return albu.Compose([
        albu.HorizontalFlip(p=0.5),
        albu.ShiftScaleRotate(scale_limit=0.1, rotate_limit=10, shift_limit=0.1, p=0.5, border_mode=0),
        albu.RandomBrightnessContrast(p=0.2),
        albu.GaussNoise(p=0.1),
        albu.Perspective(p=0.5),
    ])


def get_preprocessing(preprocessing_fn):
    def to_tensor(x, **kwargs): return x.transpose(2, 0, 1).astype('float32')

    def transform(image, mask):
        image = preprocessing_fn(image)
        image = to_tensor(image)
        mask = torch.from_numpy(mask).long()
        return {"image": image, "mask": mask}

    return transform


# ================= 4. ä¸»ç¨‹åº =================
if __name__ == '__main__':
    print(f"========== ğŸš€ å¯åŠ¨ CamVid è®­ç»ƒ (5åˆ†ç±»ç‰ˆ) ==========")
    print(f"ğŸ“‚ æ¨¡å‹å°†ä¿å­˜ä¸º: {MODEL_SAVE_PATH}")

    x_train_dir = os.path.join(DATA_DIR, 'train')
    y_train_dir = os.path.join(DATA_DIR, 'train_labels')
    if not os.path.exists(x_train_dir):
        print("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° CamVid è·¯å¾„")
        exit()

    prep_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)

    train_dataset = CamVidDataset(
        x_train_dir, y_train_dir,
        augmentation=get_training_augmentation(),
        preprocessing=get_preprocessing(prep_fn)
    )
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)

    # æ„å»ºæ¨¡å‹
    if os.path.exists(MODEL_SAVE_PATH):
        print(f"ğŸ”„ å‘ç°å·²å­˜åœ¨çš„5åˆ†ç±»æ¨¡å‹ï¼Œæ­£åœ¨åŠ è½½: {MODEL_SAVE_PATH}")
        model = torch.load(MODEL_SAVE_PATH, weights_only=False)
    else:
        print("âœ¨ æœªå‘ç°åŒåæ¨¡å‹ï¼Œåˆ›å»ºå…¨æ–°çš„ 5åˆ†ç±» æ¨¡å‹...")
        # === ä¿®æ”¹ç‚¹ 2: ç±»åˆ«æ•°æ”¹ä¸º 5 ===
        model = smp.DeepLabV3Plus(
            encoder_name=ENCODER, encoder_weights=ENCODER_WEIGHTS,
            classes=5, activation=None
        )
    model.to(DEVICE)

    loss_fn = smp.losses.DiceLoss(mode='multiclass', from_logits=True)
    optimizer = torch.optim.Adam(model.parameters(), lr=LR)

    min_loss = float('inf')

    for epoch in range(EPOCHS):
        model.train()
        epoch_loss = 0
        print(f"\nEpoch {epoch + 1}/{EPOCHS} ...")

        for i, (images, masks) in enumerate(train_loader):
            images, masks = images.to(DEVICE), masks.to(DEVICE)
            optimizer.zero_grad()
            outputs = model(images)
            loss = loss_fn(outputs, masks)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

            if i % 20 == 0:
                print(f"  Step {i}/{len(train_loader)} | Loss: {loss.item():.4f}")

        avg_loss = epoch_loss / len(train_loader)
        print(f"âœ… Epoch {epoch + 1} ç»“æŸ | Avg Loss: {avg_loss:.4f}")

        if avg_loss < min_loss:
            min_loss = avg_loss
            torch.save(model, MODEL_SAVE_PATH)
            print(f"  ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {MODEL_SAVE_PATH} (Loss: {min_loss:.4f})")