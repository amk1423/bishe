import os
import torch
import cv2
import numpy as np
import segmentation_models_pytorch as smp
import albumentations as albu
from torch.utils.data import DataLoader
from torch.utils.data import Dataset as BaseDataset

# ================= 1. å…¨å±€é…ç½® =================
DATA_DIR = './dataset/camvid'  # è¯·ç¡®è®¤æ‚¨çš„æ•°æ®è·¯å¾„æ˜¯å¦æ­£ç¡®
ENCODER = 'mobilenet_v2'
ENCODER_WEIGHTS = 'imagenet'
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

# æ˜¾å­˜å¦‚æœä¸å¤Ÿï¼ˆæŠ¥é”™ OOMï¼‰ï¼ŒæŠŠè¿™é‡Œæ”¹æˆ 4
BATCH_SIZE = 8
LR = 0.0001
EPOCHS = 500
INPUT_HEIGHT = 384
INPUT_WIDTH = 480

MODEL_SAVE_PATH = './best_model_camvid_5classes.pth'

# ================= 2. ç±»åˆ«æ˜ å°„é…ç½® (5åˆ†ç±») =================
# 0=èƒŒæ™¯(å…¶ä»–æ‰€æœ‰), 1=è·¯, 2=äºº, 3=è½¦, 4=éª‘è¡Œè€…
CAMVID_MAPPING = {
    'road': 1, 'lane_marking_driving': 1,
    'pedestrian': 2, 'child': 2,
    'bicyclist': 4,       # å•ç‹¬åˆ†ç±»
    'car': 3, 'truck': 3, 'bus': 3, 'train': 3, 'heavy_vehicle': 3, 'pickup_truck': 3, 'van': 3
}

# CamVid åŸå§‹æ ‡ç­¾é¡ºåºï¼ˆå¿…é¡»ä¸æ•°æ®é›†ä¸€è‡´ï¼‰
CAMVID_CLASSES = ['sky', 'building', 'pole', 'road', 'pavement',
                  'tree', 'signsymbol', 'fence', 'car', 'pedestrian', 'bicyclist', 'unlabelled']

# ================= 3. æ•°æ®é›†ç±» =================
class CamVidDataset(BaseDataset):
    def __init__(self, images_dir, masks_dir, augmentation=None, preprocessing=None):
        self.ids = os.listdir(images_dir)
        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]
        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]
        self.augmentation = augmentation
        self.preprocessing = preprocessing

        # ç®€å•çš„å†…å­˜ç¼“å­˜
        self.images_cache = [None] * len(self.ids)
        self.masks_cache = [None] * len(self.ids)

    def __getitem__(self, i):
        # 1. è¯»å–å›¾åƒä¸æ ‡ç­¾ï¼ˆå¸¦ç¼“å­˜ï¼‰
        if self.images_cache[i] is not None:
            image = self.images_cache[i]
            mask = self.masks_cache[i]
        else:
            image = cv2.imread(self.images_fps[i])
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            mask = cv2.imread(self.masks_fps[i], 0) # ç°åº¦è¯»å–

            image = cv2.resize(image, (INPUT_WIDTH, INPUT_HEIGHT), interpolation=cv2.INTER_LINEAR)
            mask = cv2.resize(mask, (INPUT_WIDTH, INPUT_HEIGHT), interpolation=cv2.INTER_NEAREST)

            self.images_cache[i] = image
            self.masks_cache[i] = mask

        # 2. ç”Ÿæˆç›®æ ‡æ©ç  (Mapping)
        target_mask = np.zeros((INPUT_HEIGHT, INPUT_WIDTH), dtype=np.uint8)
        
        for class_name, target_id in CAMVID_MAPPING.items():
            if class_name in CAMVID_CLASSES:
                src_id = CAMVID_CLASSES.index(class_name)
                target_mask[mask == src_id] = target_id

        # ============ ã€å…³é”®ä¿®å¤ã€‘ ============
        # å¼ºåˆ¶è½¬æ¢ä¸º uint8ï¼Œé˜²æ­¢ OpenCV æŠ¥é”™ "int64 is not supported"
        target_mask = target_mask.astype(np.uint8)
        # ====================================

        # 3. æ•°æ®å¢å¼º
        if self.augmentation:
            sample = self.augmentation(image=image, mask=target_mask)
            image, target_mask = sample['image'], sample['mask']

        # 4. é¢„å¤„ç†
        if self.preprocessing:
            sample = self.preprocessing(image=image, mask=target_mask)
            image, target_mask = sample['image'], sample['mask']

        return image, target_mask

    def __len__(self):
        return len(self.ids)

# ================= 4. è¾…åŠ©å‡½æ•° =================
def get_training_augmentation():
    return albu.Compose([
        albu.HorizontalFlip(p=0.5),
        albu.ShiftScaleRotate(scale_limit=0.1, rotate_limit=10, shift_limit=0.1, p=0.5, border_mode=0),
        albu.RandomBrightnessContrast(p=0.2),
        albu.GaussNoise(p=0.1),
        albu.Perspective(p=0.5),
    ])

def get_preprocessing(preprocessing_fn):
    def to_tensor(x, **kwargs):
        return x.transpose(2, 0, 1).astype('float32')

    def transform(image, mask):
        image = preprocessing_fn(image)
        image = to_tensor(image)
        mask = torch.from_numpy(mask).long()
        return {"image": image, "mask": mask}

    return transform

# ================= 5. ä¸»è®­ç»ƒå¾ªç¯ =================
if __name__ == '__main__':
    print(f"========== ğŸš€ å¯åŠ¨ CamVid è®­ç»ƒ (5åˆ†ç±»ç‰ˆ) ==========")
    print(f"âš™ï¸  è®¾å¤‡: {DEVICE}")
    print(f"ğŸ“‚ æ¨¡å‹ä¿å­˜è·¯å¾„: {MODEL_SAVE_PATH}")

    x_train_dir = os.path.join(DATA_DIR, 'train')
    y_train_dir = os.path.join(DATA_DIR, 'train_labels')

    if not os.path.exists(x_train_dir):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®é›†è·¯å¾„: {x_train_dir}")
        print("è¯·æ£€æŸ¥ DATA_DIR å˜é‡è®¾ç½®æ˜¯å¦æ­£ç¡®ã€‚")
        exit()

    # è·å–é¢„å¤„ç†å‡½æ•°
    try:
        prep_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)
    except:
        print("âš ï¸ æ— æ³•è·å–é¢„å¤„ç†å‡½æ•°ï¼ˆç½‘ç»œé—®é¢˜ï¼Ÿï¼‰ï¼Œä½¿ç”¨é»˜è®¤å¤„ç†ã€‚")
        prep_fn = smp.encoders.get_preprocessing_fn(ENCODER, "imagenet")

    # åˆ›å»º Dataset å’Œ DataLoader
    train_dataset = CamVidDataset(
        x_train_dir, y_train_dir,
        augmentation=get_training_augmentation(),
        preprocessing=get_preprocessing(prep_fn)
    )
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)

    # æ¨¡å‹åŠ è½½/åˆ›å»º
    if os.path.exists(MODEL_SAVE_PATH):
        print(f"ğŸ”„ åŠ è½½å·²æœ‰æ¨¡å‹ç»§ç»­è®­ç»ƒ: {MODEL_SAVE_PATH}")
        model = torch.load(MODEL_SAVE_PATH, weights_only=False)
    else:
        print("âœ¨ åˆ›å»ºå…¨æ–° DeepLabV3+ (MobileNetV2) æ¨¡å‹...")
        try:
            model = smp.DeepLabV3Plus(
                encoder_name=ENCODER, 
                encoder_weights=ENCODER_WEIGHTS, 
                classes=5, 
                activation=None
            )
        except Exception as e:
            print(f"âš ï¸ é¢„è®­ç»ƒæƒé‡ä¸‹è½½å¤±è´¥ ({e})ï¼Œæ­£åœ¨å°è¯•ä¸ä½¿ç”¨é¢„è®­ç»ƒæƒé‡å¯åŠ¨...")
            model = smp.DeepLabV3Plus(
                encoder_name=ENCODER, 
                encoder_weights=None, 
                classes=5, 
                activation=None
            )

    model.to(DEVICE)

    # æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨
    loss_fn = smp.losses.DiceLoss(mode='multiclass', from_logits=True)
    optimizer = torch.optim.Adam(model.parameters(), lr=LR)

    min_loss = float('inf')

    # å¼€å§‹è®­ç»ƒ
    for epoch in range(EPOCHS):
        model.train()
        epoch_loss = 0
        print(f"\nEpoch {epoch + 1}/{EPOCHS} ...")

        for i, (images, masks) in enumerate(train_loader):
            images, masks = images.to(DEVICE), masks.to(DEVICE)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = loss_fn(outputs, masks)
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

            if i % 10 == 0:
                print(f"  Step {i}/{len(train_loader)} | Loss: {loss.item():.4f}")

        avg_loss = epoch_loss / len(train_loader)
        print(f"âœ… Epoch {epoch + 1} å®Œæˆ | Avg Loss: {avg_loss:.4f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_loss < min_loss:
            min_loss = avg_loss
            torch.save(model, MODEL_SAVE_PATH)
            print(f"  ğŸ’¾ æ¨¡å‹å·²æ›´æ–°ï¼ŒLoss é™è‡³: {min_loss:.4f}")