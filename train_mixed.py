import os
import torch
import cv2
import numpy as np
import segmentation_models_pytorch as smp
import albumentations as albu
from torch.utils.data import DataLoader, ConcatDataset
from torch.utils.data import Dataset as BaseDataset

# ================= 1. å…¨å±€é…ç½® =================
CAMVID_DIR = './dataset/camvid'
CITY_DIR = './dataset/cityscapes'

ENCODER = 'mobilenet_v2'
ENCODER_WEIGHTS = 'imagenet'
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

BATCH_SIZE = 6
LR = 0.0001
EPOCHS = 50
INPUT_HEIGHT = 384
INPUT_WIDTH = 480

MODEL_SAVE_PATH = './best_model_mixed.pth'

# ================= 2. CamVid æ˜ å°„ (ä¿æŒä¸å˜) =================
CAMVID_MAPPING = {
    'road': 1, 'lane_marking_driving': 1,
    'pedestrian': 2, 'bicyclist': 2, 'child': 2,
    'car': 3, 'truck': 3, 'bus': 3, 'train': 3, 'heavy_vehicle': 3, 'pickup_truck': 3, 'van': 3
}
CAMVID_CLASSES = ['sky', 'building', 'pole', 'road', 'pavement',
                  'tree', 'signsymbol', 'fence', 'car', 'pedestrian', 'bicyclist', 'unlabelled']


# ================= 3. æ•°æ®é›†ç±» (æ ¸å¿ƒå‡çº§ï¼šæ¨¡ç³ŠåŒ¹é…) =================
class SegmentationDataset(BaseDataset):
    def __init__(self, images_dir, masks_dir, source_type='camvid', augmentation=None, preprocessing=None):
        self.ids = os.listdir(images_dir)
        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]
        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]
        self.source_type = source_type
        self.augmentation = augmentation
        self.preprocessing = preprocessing

        # å†…å­˜ç¼“å­˜
        self.images_cache = [None] * len(self.ids)
        self.masks_cache = [None] * len(self.ids)

    def __getitem__(self, i):
        # 1. ç¼“å­˜è¯»å–
        if self.images_cache[i] is not None:
            image = self.images_cache[i]
            mask = self.masks_cache[i]
        else:
            image = cv2.imread(self.images_fps[i])
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            if self.source_type == 'cityscapes':
                mask = cv2.imread(self.masks_fps[i])
                mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)
            else:
                mask = cv2.imread(self.masks_fps[i], 0)

            image = cv2.resize(image, (INPUT_WIDTH, INPUT_HEIGHT), interpolation=cv2.INTER_LINEAR)
            mask = cv2.resize(mask, (INPUT_WIDTH, INPUT_HEIGHT), interpolation=cv2.INTER_NEAREST)

            self.images_cache[i] = image
            self.masks_cache[i] = mask

        # 2. æ ‡ç­¾è½¬æ¢ (è¿™é‡Œç”¨äº†æ›´æ™ºèƒ½çš„æ¨¡ç³ŠåŒ¹é…ï¼)
        target_mask = np.zeros((INPUT_HEIGHT, INPUT_WIDTH), dtype=np.longlong)

        if self.source_type == 'camvid':
            for class_name, target_id in CAMVID_MAPPING.items():
                if class_name in CAMVID_CLASSES:
                    src_id = CAMVID_CLASSES.index(class_name)
                    target_mask[mask == src_id] = target_id

        elif self.source_type == 'cityscapes':
            # === æ ¸å¿ƒå‡çº§ï¼šRGB èŒƒå›´åŒ¹é… (å®¹é”™ +/- 10) ===
            # æˆ‘ä»¬ä¸å†åŒ¹é…å…·ä½“çš„ (128,64,128)ï¼Œè€Œæ˜¯åŒ¹é…ä¸€ä¸ªâ€œç´«è‰²èŒƒå›´â€

            # 1. è·¯ (ç´«è‰²ç³») - æ ‡å‡†(128, 64, 128)
            # åªè¦ Råœ¨118-138, Gåœ¨54-74, Båœ¨118-138 ä¹‹é—´ï¼Œéƒ½ç®—è·¯ï¼
            is_road = (mask[:, :, 0] > 118) & (mask[:, :, 0] < 138) & \
                      (mask[:, :, 1] > 54) & (mask[:, :, 1] < 74) & \
                      (mask[:, :, 2] > 118) & (mask[:, :, 2] < 138)
            target_mask[is_road] = 1

            # 2. äºº (çº¢è‰²ç³») - æ ‡å‡†(220, 20, 60)
            is_person = (mask[:, :, 0] > 200) & \
                        (mask[:, :, 1] < 50) & \
                        (mask[:, :, 2] < 100)
            target_mask[is_person] = 2

            # 3. è½¦ (æ·±çº¢/æ·±è“ç³»)
            # ä½ çš„æ•°æ®é›†è½¦æ˜¯æ·±çº¢ (142, 0, 0)
            is_car_red = (mask[:, :, 0] > 130) & (mask[:, :, 0] < 160) & \
                         (mask[:, :, 1] < 30) & \
                         (mask[:, :, 2] < 30)
            # å…¼å®¹æ ‡å‡†ç‰ˆçš„æ·±è“ (0, 0, 142)
            is_car_blue = (mask[:, :, 2] > 130) & (mask[:, :, 2] < 160) & \
                          (mask[:, :, 0] < 30) & \
                          (mask[:, :, 1] < 30)

            target_mask[is_car_red | is_car_blue] = 3

        # 3. å¢å¼ºä¸é¢„å¤„ç†
        if self.augmentation:
            sample = self.augmentation(image=image, mask=target_mask)
            image, target_mask = sample['image'], sample['mask']
        if self.preprocessing:
            sample = self.preprocessing(image=image, mask=target_mask)
            image, target_mask = sample['image'], sample['mask']

        return image, target_mask

    def __len__(self):
        return len(self.ids)


def get_training_augmentation():
    return albu.Compose([
        albu.HorizontalFlip(p=0.5),
        albu.RandomBrightnessContrast(p=0.2),
        albu.ShiftScaleRotate(scale_limit=0.05, rotate_limit=5, shift_limit=0.05, p=0.5, border_mode=0),
    ])


# âœ… æ›¿æ¢æˆè¿™ä¸ªï¼ˆå®šä¹‰ä¸€ä¸ªå¯åºåˆ—åŒ–çš„ç±»ï¼‰
class PreprocessingTransform:
    def __init__(self, preprocessing_fn):
        self.preprocessing_fn = preprocessing_fn

    def __call__(self, image, mask):
        # 1. å›¾åƒé¢„å¤„ç† (å½’ä¸€åŒ–)
        image = self.preprocessing_fn(image)
        # 2. è½¬ç½® (H,W,C) -> (C,H,W)
        image = image.transpose(2, 0, 1).astype('float32')
        # 3. æ ‡ç­¾è½¬ LongTensor
        mask = torch.from_numpy(mask).long()
        return {"image": image, "mask": mask}

def get_preprocessing(preprocessing_fn):
    return PreprocessingTransform(preprocessing_fn)


# ================= 4. ä¸»ç¨‹åº =================
if __name__ == '__main__':
    print("========== ğŸš€ å¯åŠ¨æŠ—å¹²æ‰°æ··åˆè®­ç»ƒ (é€‚é…ä¸€åˆ‡é¢œè‰²åå·®) ==========")

    prep_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)

    # åŠ è½½æ•°æ®é›†
    ds_camvid = SegmentationDataset(
        os.path.join(CAMVID_DIR, 'train'), os.path.join(CAMVID_DIR, 'train_labels'),
        source_type='camvid', augmentation=get_training_augmentation(), preprocessing=get_preprocessing(prep_fn)
    )

    full_dataset = ds_camvid
    if os.path.exists(CITY_DIR):
        print(f"æ£€æµ‹åˆ° Cityscapesï¼Œæ­£åœ¨åˆå¹¶...")
        ds_city = SegmentationDataset(
            os.path.join(CITY_DIR, 'train'), os.path.join(CITY_DIR, 'train_labels'),
            source_type='cityscapes', augmentation=get_training_augmentation(), preprocessing=get_preprocessing(prep_fn)
        )
        full_dataset = ConcatDataset([ds_camvid, ds_city])
        print(f"âœ… åˆå¹¶æˆåŠŸï¼")

    print(f"æ€»å›¾ç‰‡æ•°: {len(full_dataset)}")
    # å¼€å¯ pin_memory åŠ é€Ÿï¼Œæ˜¾å­˜å…è®¸å¯å¼€ num_workers=2
    train_loader = DataLoader(full_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)

    # æ¨¡å‹ä¸ä¼˜åŒ–å™¨
    if os.path.exists(MODEL_SAVE_PATH):
        print(f"ğŸ”„ åŠ è½½å­˜æ¡£: {MODEL_SAVE_PATH}")
        model = torch.load(MODEL_SAVE_PATH, weights_only=False)
    else:
        print("âœ¨ åˆ›å»ºæ–°æ¨¡å‹...")
        model = smp.DeepLabV3Plus(
            encoder_name=ENCODER, encoder_weights=ENCODER_WEIGHTS,
            classes=4, activation=None
        )
    model.to(DEVICE)

    loss_fn = smp.losses.DiceLoss(mode='multiclass', from_logits=True)
    optimizer = torch.optim.Adam(model.parameters(), lr=LR)

    min_loss = float('inf')

    for epoch in range(EPOCHS):
        model.train()
        epoch_loss = 0
        print(f"\nEpoch {epoch + 1}/{EPOCHS} ...")

        for i, (images, masks) in enumerate(train_loader):
            images, masks = images.to(DEVICE), masks.to(DEVICE)
            optimizer.zero_grad()
            outputs = model(images)
            loss = loss_fn(outputs, masks)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

            if i % 50 == 0:
                print(f"  Step {i}/{len(train_loader)} | Loss: {loss.item():.4f}")

        avg_loss = epoch_loss / len(train_loader)
        print(f"âœ… Epoch {epoch + 1} ç»“æŸ | Avg Loss: {avg_loss:.4f}")

        if avg_loss < min_loss:
            min_loss = avg_loss
            torch.save(model, MODEL_SAVE_PATH)
            print(f"  ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜! (Loss: {min_loss:.4f})")