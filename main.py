import torch

print("========== æ˜¾å¡æ¿€æ´»æµ‹è¯• ==========")

# 1. çœ‹çœ‹æ˜¯ä¸æ˜¯ GPU ç‰ˆæœ¬
print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")

# 2. å…³é”®æ—¶åˆ»ï¼šæ˜¾å¡èƒ½ä¸èƒ½ç”¨ï¼Ÿ
if torch.cuda.is_available():
    print("âœ… æˆåŠŸï¼æ˜¾å¡å·²æ¿€æ´»ï¼(NVIDIA CUDA is ready)")
    print(f"ğŸš€ å½“å‰æ˜¾å¡å‹å·: {torch.cuda.get_device_name(0)}")

    # æµ‹è¯•ä¸€ä¸‹æ˜¾å­˜
    mem = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3
    print(f"ğŸ’¾ æ˜¾å­˜å¤§å°: {mem:.2f} GB")

    # åšä¸ª GPU è¿ç®—æµ‹è¯•
    a = torch.rand(1000, 1000).to('cuda')
    b = torch.rand(1000, 1000).to('cuda')
    print("âš¡ GPU è®¡ç®—æµ‹è¯•é€šè¿‡ï¼")
else:
    print("âŒ å¤±è´¥... å½“å‰ä¾ç„¶æ˜¯ CPU æ¨¡å¼")
    print("è¯·æ£€æŸ¥æ˜¯å¦å®‰è£…äº†æ­£ç¡®çš„ CUDA ç‰ˆæœ¬ PyTorch")

print("==================================")